#!/usr/bin/env bash

accelerate launch --num_processes=1 --gpu_ids="1" run_distillation.py \
  --model_name_or_path "sanchit-gandhi/tiny-random-MistralForCausalLM-1-layer" \
  --teacher_model_name_or_path "sanchit-gandhi/tiny-random-MistralForCausalLM-1-layer" \
  --output_dir "./" \
  --train_dataset_name "sanchit-gandhi/comsopedia-100k-logprobs" \
  --train_dataset_config_name "default" \
	--train_split_name "train[1000:]" \
	--eval_split_name "train[:1000]" \
	--preprocessing_num_workers "32" \
	--do_train \
  --do_eval \
  --prompt_column_name "prompt" \
  --eval_prompt_column_name "prompt" \
	--max_train_samples 100 \
  --num_train_epochs 2 \
  --max_eval_samples 100 \
  --per_device_eval_batch_size 8 \
  --per_device_train_batch_size 8 \
  --logprob_threshold -2.0 \
  --save_strategy "no" \
	--evaluation_strategy "epoch" \
	--logging_steps 1 \
  --overwrite_output_dir \
  --output_router_logits True \
  --report_to "wandb" \
  --load_teacher_in_4bit \
  --gradient_checkpointing \
  --dtype "bfloat16" \
  --wandb_project "distil-mixtral-dummy"
